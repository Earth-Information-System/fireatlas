{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3932eb24-10cc-4efc-890b-b5afe57be5ac",
   "metadata": {},
   "source": [
    "# Canadian fire\n",
    "\n",
    "\n",
    "This notebook runs all the preprocessing steps using NRT data for a specific region and time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b1006-0611-41c6-9bf9-35b1dbc07dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed the fireatlas code yet, uncomment the following line and run this cell.\n",
    "\n",
    "!pip install -e .. -q\n",
    "\n",
    "# After this runs, restart the notebook kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d030e-eabd-4ed1-b0a1-82b3602f9190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from fireatlas import FireIO, FireMain, FireTime, FireObj, postprocess, preprocess, settings\n",
    "from fireatlas.utils import timed\n",
    "\n",
    "tst = [2023, 6, 1, 'AM']\n",
    "ted = [2023, 8, 12, 'PM']\n",
    "bbox = [-76.510259,  52.624922, -73.801554,  53.743852]\n",
    "region = ('Issue2', bbox)\n",
    "list_of_ts = list(FireTime.t_generator(tst, ted))\n",
    "\n",
    "settings.OUTPUT_DIR = \"take2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54a2d4-b41b-4d53-a4ba-7a41f02799f5",
   "metadata": {},
   "source": [
    "## Preprocess the region\n",
    "\n",
    "Preprocess the region to get rid of static flare sources. Save that new \"swiss cheese\" shape off into a geojson file for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c495d-66fa-458f-8afb-01e982b0709a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess.preprocess_region(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd857f-f57a-4fc3-9e49-2c9536c97019",
   "metadata": {},
   "source": [
    "## Are preprocessed input files available?\n",
    "\n",
    "Check to see if all the files that we need have already been processed and are available on s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451d5d6-05b0-4393-a850-70758cd365db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts_that_need_preprocessing = set([\n",
    "    *preprocess.check_preprocessed_file(tst, ted, sat=\"NOAA20\", freq=\"NRT\", location=\"s3\"),\n",
    "    *preprocess.check_preprocessed_file(tst, ted, sat=\"SNPP\", freq=\"NRT\", location=\"s3\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10e2b4-a8a1-4f26-943e-0779b140c4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(ts_that_need_preprocessing) == 0, \"All input fires should be on s3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9065d-3519-4673-98e5-3afd0e29ab80",
   "metadata": {},
   "source": [
    "## Preprocess for region and t\n",
    "\n",
    "Do initial filtering and clustering using the preprocessed region (from local) and the half day files (from s3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3b8b4-7330-48e4-a3ff-4ca4d56add2c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for t in list_of_ts:\n",
    "    preprocess.preprocess_region_t(t, region=region, read_location=\"s3\", read_region_location=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a2dc9-e4ba-46ea-9e35-fca95f4514b0",
   "metadata": {},
   "source": [
    "## Run the fire expansion and merging algorithm\n",
    "\n",
    "Now that we have all the region-specific preprocessed fires, we can go ahead and run `Fire_Forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9177ca7-7ddb-4472-97a2-054467bf86f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "allfires, allpixels = FireMain.Fire_Forward(tst=tst, ted=ted, restart=True, region=region, read_location=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0d531-3a13-4881-846a-8d6c8dd1e7fc",
   "metadata": {},
   "source": [
    "## Take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862ad97-1304-440b-b8ec-d8cb67251ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdde580-2bf6-4d26-85c8-6a6d14dc94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = preprocess.read_region(region, location=\"local\")\n",
    "\n",
    "gdf = postprocess.read_allfires_gdf(tst, ted, region, location=\"local\").reset_index()\n",
    "gdf = postprocess.fill_activefire_rows(gdf, ted)\n",
    "\n",
    "gdf_fid = gdf[(gdf.fireID == 1) | (gdf.mergeid == 1)]\n",
    "merged = postprocess.merge_rows(gdf_fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefcad7-6605-42e7-b45e-e8c7cbf19f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymap = folium.Map(max_bounds=region[1])\n",
    "mymap.fit_bounds([[bbox[1], bbox[0]], [bbox[3], bbox[2]]])\n",
    "\n",
    "# put the region on the map\n",
    "folium.GeoJson(\n",
    "    region[1],\n",
    "    style_function=lambda x: {'fillColor': 'pink', 'color': 'red', 'weight': 2}\n",
    ").add_to(mymap)\n",
    "\n",
    "for i, t in enumerate(gdf.t.unique()[::-1]):\n",
    "    data = merged[(merged[\"t\"] == t)]\n",
    "    folium.GeoJson(\n",
    "        data[\"hull\"],\n",
    "        name=f\"merged {t}\",\n",
    "        color=\"green\" if i % 2 else \"purple\" ,\n",
    "        fill_opacity=0.1,\n",
    "        weight=2,\n",
    "        show=False\n",
    "    ).add_to(mymap)\n",
    "    \n",
    "    # add allfires perimeters\n",
    "    data = gdf[(gdf[\"t\"] == t) & (~gdf[\"invalid\"])]\n",
    "    folium.GeoJson(\n",
    "        data[\"hull\"],\n",
    "        name=f\"cluster hulls {t}\",\n",
    "        color=\"red\" if i % 2 else \"blue\" ,\n",
    "        fill_opacity=0.1,\n",
    "        weight=2,\n",
    "        show=False\n",
    "    ).add_to(mymap)\n",
    "\n",
    "mymap.add_child(folium.LayerControl(collapsed=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ae292-10fe-40df-ac76-5670c2c48f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[[\"t\", \"farea\", \"n_newpixels\"]].hvplot(x=\"t\", y=\"farea\", secondary_y=\"n_newpixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48478f-189a-455d-b9ca-c323b46054e4",
   "metadata": {},
   "source": [
    "## Write Snapshot files\n",
    "\n",
    "Start by reading in the outputs from the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41cbc8-f2c0-4b03-b9bc-d360a2bdd231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allpixels = postprocess.read_allpixels(tst, ted, region, location=\"local\")\n",
    "allfires_gdf = postprocess.read_allfires_gdf(tst, ted, region, location=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08a1bb9-74fe-4d91-878d-89709f0c9548",
   "metadata": {},
   "source": [
    "Then iterate through all of the ts and save the snapshot layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fe52c-793d-4f9a-9bd7-8cf0421daecb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "postprocess.save_snapshots(allfires_gdf, region, tst, ted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ffd2d-ed4b-498e-b45e-127f7c5fd594",
   "metadata": {},
   "source": [
    "## Write Largefire files\n",
    "\n",
    "First we will find all the large fires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130838af-f05c-48fe-b52f-1e8d0a5d6747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "large_fires = postprocess.find_largefires(allfires_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d6185-c72b-47c9-abe2-c4e9cf71681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r data.zip ~/fireatlas_nrt/data/take2/Issue2/2023/Largefire/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab6685-4dec-4ad6-9e0a-31f69db864cd",
   "metadata": {},
   "source": [
    "First we'll use the `allpixels` object to create the `nplist` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718c1ae-99f2-4edd-9e89-a101279e3515",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "postprocess.save_large_fires_nplist(allpixels, region, large_fires, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308bd400-ae4e-40aa-86b0-8f0986d34062",
   "metadata": {},
   "source": [
    "The rest of the layers will be created directly from the `allfires_gdf` and have some merge handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f77f34-e642-4de7-872c-3ba8b7d9a4c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "postprocess.save_large_fires_layers(allfires_gdf, region, large_fires, tst, ted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
